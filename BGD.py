# -*- coding: utf-8 -*-
"""1910040_Tasnia_CSE425_final_assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SWdTB862-2fbqvpReEUyoABa0krGgONi
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

data = pd.read_csv("data.csv")
X = data[['Volume', 'Weight']]
y = data['CO2']

print(X)

volume = data['Volume']
weight = data['Weight']
print(weight)
print(volume)

#2a...normalizing weight
min_weight= weight.min()
#print(min_weight)
max_weight= weight.max()
#print(max_weight)
for i in range(0,36):
  weight[i]= (weight[i]-min_weight)/(max_weight-min_weight) 
print(weight)

#2a...normalizing volume
min_vol= volume.min()
print(min_vol)
max_vol= volume.max()
print(max_vol)
for i in range(0,36):
  volume[i]= (volume[i]-min_vol)/(max_vol-min_vol) 
print(volume)

X = pd.merge(volume,weight, suffixes=['Volume','Weight'],left_index= True, right_index=True)
# print(X)
X = X.to_numpy()
# print(X)

# slicing the data into train and test sets
X_train = X[:30]
#print('xtrain',X_train)

y_train = y[:30]
#print('ytrain', y_train)

X_test = X[30:]
#print('Xtest', X_test)

y_test = y[30:]

#print(y_test)
class LinearRegression:
    def __init__(self, learning_rate=0.01):
        self.learning_rate = learning_rate
    
    def fit(self, X, y, epochs=100, batch_size=10):
        n, d = X.shape
        self.W = np.random.randn(d)
        self.b = 0
        self.losses = []
        for epoch in range(epochs):
            
            index = np.random.permutation(n) #shuffle
            X = X[index]
            y = y[index]
            # Split the data into batches
            for i in range(0, n, batch_size):
                X_batch = X[i:i+batch_size]
                y_batch = y[i:i+batch_size]
                #gradient of the loss(del J) based on W and b
                y_pred = X_batch.dot(self.W) + self.b
                delta = y_pred - y_batch
                dW = X_batch.T.dot(delta) / batch_size
                db = delta.mean(axis=0)
                
                self.W -= self.learning_rate * dW
                self.b -= self.learning_rate * db

            # Compute the loss
            y_pred = X.dot(self.W) + self.b
            loss = ((y_pred - y)**2).mean() / 2
            self.losses.append(loss)
    
    def predict(self, X):
        print("Predicted CO2 emissions",X.dot(self.W) + self.b)
        return X.dot(self.W) + self.b

#Batch Gradient Descent

bgd = LinearRegression(learning_rate=0.1)
bgd.fit(X_train, y_train, epochs=100, batch_size=len(X_train))
y_pred_bgd = bgd.predict(X_test)

mse_bgd = ((y_pred_bgd - y_test)**2).mean()
print("MSE for Batch Gradient Descent is",mse_bgd)


plt.plot(bgd.losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')

plt.show()

